{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.6260162601626016,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016260162601626018,
      "grad_norm": 4.102899074554443,
      "learning_rate": 0.0001989159891598916,
      "loss": 2.4242,
      "step": 10
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 2.351649284362793,
      "learning_rate": 0.00019783197831978321,
      "loss": 1.5107,
      "step": 20
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 3.220911741256714,
      "learning_rate": 0.00019674796747967482,
      "loss": 1.1754,
      "step": 30
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 4.484182357788086,
      "learning_rate": 0.0001956639566395664,
      "loss": 0.857,
      "step": 40
    },
    {
      "epoch": 0.08130081300813008,
      "grad_norm": 4.215541839599609,
      "learning_rate": 0.000194579945799458,
      "loss": 0.5763,
      "step": 50
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 3.953232526779175,
      "learning_rate": 0.00019349593495934962,
      "loss": 0.42,
      "step": 60
    },
    {
      "epoch": 0.11382113821138211,
      "grad_norm": 3.713036298751831,
      "learning_rate": 0.0001924119241192412,
      "loss": 0.2877,
      "step": 70
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 2.6591408252716064,
      "learning_rate": 0.0001913279132791328,
      "loss": 0.1769,
      "step": 80
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 4.053531646728516,
      "learning_rate": 0.0001902439024390244,
      "loss": 0.144,
      "step": 90
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 2.823812246322632,
      "learning_rate": 0.000189159891598916,
      "loss": 0.1395,
      "step": 100
    },
    {
      "epoch": 0.17886178861788618,
      "grad_norm": 2.3399741649627686,
      "learning_rate": 0.0001880758807588076,
      "loss": 0.1384,
      "step": 110
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 2.393558979034424,
      "learning_rate": 0.00018699186991869918,
      "loss": 0.1344,
      "step": 120
    },
    {
      "epoch": 0.21138211382113822,
      "grad_norm": 2.055365800857544,
      "learning_rate": 0.00018590785907859078,
      "loss": 0.122,
      "step": 130
    },
    {
      "epoch": 0.22764227642276422,
      "grad_norm": 1.8710949420928955,
      "learning_rate": 0.0001848238482384824,
      "loss": 0.1284,
      "step": 140
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 2.704667806625366,
      "learning_rate": 0.000183739837398374,
      "loss": 0.1335,
      "step": 150
    },
    {
      "epoch": 0.2601626016260163,
      "grad_norm": 2.3714263439178467,
      "learning_rate": 0.0001826558265582656,
      "loss": 0.1175,
      "step": 160
    },
    {
      "epoch": 0.2764227642276423,
      "grad_norm": 2.2169430255889893,
      "learning_rate": 0.0001815718157181572,
      "loss": 0.118,
      "step": 170
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 1.9993646144866943,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.1207,
      "step": 180
    },
    {
      "epoch": 0.3089430894308943,
      "grad_norm": 1.406777024269104,
      "learning_rate": 0.0001794037940379404,
      "loss": 0.1136,
      "step": 190
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 1.347113847732544,
      "learning_rate": 0.00017831978319783197,
      "loss": 0.1162,
      "step": 200
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": NaN,
      "learning_rate": 0.0001773441734417344,
      "loss": 0.1208,
      "step": 210
    },
    {
      "epoch": 0.35772357723577236,
      "grad_norm": 1.4314002990722656,
      "learning_rate": 0.00017626016260162603,
      "loss": 0.1168,
      "step": 220
    },
    {
      "epoch": 0.37398373983739835,
      "grad_norm": 1.9479658603668213,
      "learning_rate": 0.00017517615176151764,
      "loss": 0.1105,
      "step": 230
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 1.7132841348648071,
      "learning_rate": 0.0001740921409214092,
      "loss": 0.1095,
      "step": 240
    },
    {
      "epoch": 0.4065040650406504,
      "grad_norm": 1.1401349306106567,
      "learning_rate": 0.00017300813008130081,
      "loss": 0.1109,
      "step": 250
    },
    {
      "epoch": 0.42276422764227645,
      "grad_norm": 1.4661171436309814,
      "learning_rate": 0.00017192411924119242,
      "loss": 0.1079,
      "step": 260
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 0.8936856985092163,
      "learning_rate": 0.00017084010840108402,
      "loss": 0.1053,
      "step": 270
    },
    {
      "epoch": 0.45528455284552843,
      "grad_norm": 1.0141737461090088,
      "learning_rate": 0.00016975609756097562,
      "loss": 0.1056,
      "step": 280
    },
    {
      "epoch": 0.4715447154471545,
      "grad_norm": 0.9100502729415894,
      "learning_rate": 0.0001686720867208672,
      "loss": 0.0995,
      "step": 290
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.1096041202545166,
      "learning_rate": 0.00016758807588075883,
      "loss": 0.105,
      "step": 300
    },
    {
      "epoch": 0.5040650406504065,
      "grad_norm": 1.4955430030822754,
      "learning_rate": 0.00016650406504065043,
      "loss": 0.103,
      "step": 310
    },
    {
      "epoch": 0.5203252032520326,
      "grad_norm": 1.1850271224975586,
      "learning_rate": 0.000165420054200542,
      "loss": 0.105,
      "step": 320
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 1.672602891921997,
      "learning_rate": 0.00016433604336043363,
      "loss": 0.1036,
      "step": 330
    },
    {
      "epoch": 0.5528455284552846,
      "grad_norm": 0.6685163378715515,
      "learning_rate": 0.0001632520325203252,
      "loss": 0.0886,
      "step": 340
    },
    {
      "epoch": 0.5691056910569106,
      "grad_norm": 0.8255481719970703,
      "learning_rate": 0.0001621680216802168,
      "loss": 0.0979,
      "step": 350
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 1.454077959060669,
      "learning_rate": 0.0001610840108401084,
      "loss": 0.0975,
      "step": 360
    },
    {
      "epoch": 0.6016260162601627,
      "grad_norm": 0.8690885901451111,
      "learning_rate": 0.00016,
      "loss": 0.0994,
      "step": 370
    },
    {
      "epoch": 0.6178861788617886,
      "grad_norm": 3.017439126968384,
      "learning_rate": 0.00015891598915989162,
      "loss": 0.0919,
      "step": 380
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 1.1588222980499268,
      "learning_rate": 0.0001578319783197832,
      "loss": 0.0929,
      "step": 390
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 0.6617653369903564,
      "learning_rate": 0.0001567479674796748,
      "loss": 0.0944,
      "step": 400
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.988123893737793,
      "learning_rate": 0.00015566395663956642,
      "loss": 0.0981,
      "step": 410
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.6629170179367065,
      "learning_rate": 0.000154579945799458,
      "loss": 0.0866,
      "step": 420
    },
    {
      "epoch": 0.6991869918699187,
      "grad_norm": 0.7026291489601135,
      "learning_rate": 0.0001534959349593496,
      "loss": 0.0947,
      "step": 430
    },
    {
      "epoch": 0.7154471544715447,
      "grad_norm": 0.941340982913971,
      "learning_rate": 0.0001524119241192412,
      "loss": 0.0926,
      "step": 440
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 0.8728349804878235,
      "learning_rate": 0.0001513279132791328,
      "loss": 0.0919,
      "step": 450
    },
    {
      "epoch": 0.7479674796747967,
      "grad_norm": 0.7939726114273071,
      "learning_rate": 0.0001502439024390244,
      "loss": 0.0988,
      "step": 460
    },
    {
      "epoch": 0.7642276422764228,
      "grad_norm": 0.4930836260318756,
      "learning_rate": 0.00014915989159891598,
      "loss": 0.0899,
      "step": 470
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.8693749904632568,
      "learning_rate": 0.0001480758807588076,
      "loss": 0.0872,
      "step": 480
    },
    {
      "epoch": 0.7967479674796748,
      "grad_norm": 0.9413743019104004,
      "learning_rate": 0.0001469918699186992,
      "loss": 0.0871,
      "step": 490
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 0.4732356667518616,
      "learning_rate": 0.0001459078590785908,
      "loss": 0.0837,
      "step": 500
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.6137702465057373,
      "learning_rate": 0.0001448238482384824,
      "loss": 0.0963,
      "step": 510
    },
    {
      "epoch": 0.8455284552845529,
      "grad_norm": 0.6269863843917847,
      "learning_rate": 0.000143739837398374,
      "loss": 0.0905,
      "step": 520
    },
    {
      "epoch": 0.8617886178861789,
      "grad_norm": 0.7161632776260376,
      "learning_rate": 0.0001426558265582656,
      "loss": 0.0864,
      "step": 530
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.8280112147331238,
      "learning_rate": 0.0001415718157181572,
      "loss": 0.0936,
      "step": 540
    },
    {
      "epoch": 0.8943089430894309,
      "grad_norm": 0.7666530609130859,
      "learning_rate": 0.00014048780487804877,
      "loss": 0.0828,
      "step": 550
    },
    {
      "epoch": 0.9105691056910569,
      "grad_norm": 1.0902659893035889,
      "learning_rate": 0.0001394037940379404,
      "loss": 0.0905,
      "step": 560
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 0.632888674736023,
      "learning_rate": 0.00013831978319783198,
      "loss": 0.089,
      "step": 570
    },
    {
      "epoch": 0.943089430894309,
      "grad_norm": 0.656637966632843,
      "learning_rate": 0.00013723577235772358,
      "loss": 0.0906,
      "step": 580
    },
    {
      "epoch": 0.959349593495935,
      "grad_norm": 0.5883734226226807,
      "learning_rate": 0.00013615176151761518,
      "loss": 0.0895,
      "step": 590
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.4222521185874939,
      "learning_rate": 0.00013506775067750678,
      "loss": 0.086,
      "step": 600
    },
    {
      "epoch": 0.991869918699187,
      "grad_norm": 0.7891093492507935,
      "learning_rate": 0.00013398373983739838,
      "loss": 0.0873,
      "step": 610
    },
    {
      "epoch": 1.008130081300813,
      "grad_norm": 0.45767301321029663,
      "learning_rate": 0.00013289972899728999,
      "loss": 0.0843,
      "step": 620
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 0.5901721119880676,
      "learning_rate": 0.0001318157181571816,
      "loss": 0.0872,
      "step": 630
    },
    {
      "epoch": 1.040650406504065,
      "grad_norm": 0.793926477432251,
      "learning_rate": 0.0001307317073170732,
      "loss": 0.0808,
      "step": 640
    },
    {
      "epoch": 1.056910569105691,
      "grad_norm": 0.6240178942680359,
      "learning_rate": 0.00012964769647696477,
      "loss": 0.0841,
      "step": 650
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 0.423470675945282,
      "learning_rate": 0.00012856368563685637,
      "loss": 0.0781,
      "step": 660
    },
    {
      "epoch": 1.089430894308943,
      "grad_norm": 0.6518455147743225,
      "learning_rate": 0.00012747967479674797,
      "loss": 0.0817,
      "step": 670
    },
    {
      "epoch": 1.1056910569105691,
      "grad_norm": 0.6637462377548218,
      "learning_rate": 0.00012639566395663957,
      "loss": 0.0858,
      "step": 680
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 0.5966347455978394,
      "learning_rate": 0.00012531165311653117,
      "loss": 0.0878,
      "step": 690
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 0.528919517993927,
      "learning_rate": 0.00012422764227642275,
      "loss": 0.0873,
      "step": 700
    },
    {
      "epoch": 1.1544715447154472,
      "grad_norm": 0.5259965658187866,
      "learning_rate": 0.00012314363143631438,
      "loss": 0.0779,
      "step": 710
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 0.45245394110679626,
      "learning_rate": 0.00012205962059620597,
      "loss": 0.0786,
      "step": 720
    },
    {
      "epoch": 1.1869918699186992,
      "grad_norm": 0.4419269561767578,
      "learning_rate": 0.00012097560975609757,
      "loss": 0.0789,
      "step": 730
    },
    {
      "epoch": 1.203252032520325,
      "grad_norm": 0.6723583936691284,
      "learning_rate": 0.00011989159891598916,
      "loss": 0.0847,
      "step": 740
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 0.7517113089561462,
      "learning_rate": 0.00011880758807588077,
      "loss": 0.0903,
      "step": 750
    },
    {
      "epoch": 1.2357723577235773,
      "grad_norm": 0.6618473529815674,
      "learning_rate": 0.00011772357723577236,
      "loss": 0.0853,
      "step": 760
    },
    {
      "epoch": 1.2520325203252032,
      "grad_norm": 0.6180857419967651,
      "learning_rate": 0.00011663956639566395,
      "loss": 0.0878,
      "step": 770
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 0.5499711632728577,
      "learning_rate": 0.00011555555555555555,
      "loss": 0.0801,
      "step": 780
    },
    {
      "epoch": 1.2845528455284554,
      "grad_norm": 0.5797395706176758,
      "learning_rate": 0.00011447154471544717,
      "loss": 0.0843,
      "step": 790
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 0.4798886775970459,
      "learning_rate": 0.00011338753387533876,
      "loss": 0.0797,
      "step": 800
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 0.6068078875541687,
      "learning_rate": 0.00011230352303523035,
      "loss": 0.0785,
      "step": 810
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.49681898951530457,
      "learning_rate": 0.00011121951219512196,
      "loss": 0.0833,
      "step": 820
    },
    {
      "epoch": 1.3495934959349594,
      "grad_norm": 0.5181575417518616,
      "learning_rate": 0.00011013550135501356,
      "loss": 0.0868,
      "step": 830
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.5200998187065125,
      "learning_rate": 0.00010905149051490515,
      "loss": 0.0822,
      "step": 840
    },
    {
      "epoch": 1.3821138211382114,
      "grad_norm": 0.6987287402153015,
      "learning_rate": 0.00010796747967479674,
      "loss": 0.0772,
      "step": 850
    },
    {
      "epoch": 1.3983739837398375,
      "grad_norm": 0.6116293668746948,
      "learning_rate": 0.00010688346883468836,
      "loss": 0.084,
      "step": 860
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 1.180962085723877,
      "learning_rate": 0.00010579945799457996,
      "loss": 0.0809,
      "step": 870
    },
    {
      "epoch": 1.4308943089430894,
      "grad_norm": 0.44906914234161377,
      "learning_rate": 0.00010471544715447155,
      "loss": 0.0845,
      "step": 880
    },
    {
      "epoch": 1.4471544715447155,
      "grad_norm": 0.5480754375457764,
      "learning_rate": 0.00010363143631436314,
      "loss": 0.0865,
      "step": 890
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.5531101822853088,
      "learning_rate": 0.00010254742547425475,
      "loss": 0.0859,
      "step": 900
    },
    {
      "epoch": 1.4796747967479675,
      "grad_norm": 0.4472959041595459,
      "learning_rate": 0.00010146341463414635,
      "loss": 0.0811,
      "step": 910
    },
    {
      "epoch": 1.4959349593495934,
      "grad_norm": 0.8396689891815186,
      "learning_rate": 0.00010037940379403794,
      "loss": 0.0792,
      "step": 920
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 0.5736985802650452,
      "learning_rate": 9.929539295392954e-05,
      "loss": 0.08,
      "step": 930
    },
    {
      "epoch": 1.5284552845528454,
      "grad_norm": 0.6647874116897583,
      "learning_rate": 9.821138211382113e-05,
      "loss": 0.0802,
      "step": 940
    },
    {
      "epoch": 1.5447154471544715,
      "grad_norm": 0.507353663444519,
      "learning_rate": 9.712737127371274e-05,
      "loss": 0.0857,
      "step": 950
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.5857401490211487,
      "learning_rate": 9.604336043360435e-05,
      "loss": 0.0797,
      "step": 960
    },
    {
      "epoch": 1.5772357723577235,
      "grad_norm": 0.5229634046554565,
      "learning_rate": 9.495934959349594e-05,
      "loss": 0.0835,
      "step": 970
    },
    {
      "epoch": 1.5934959349593496,
      "grad_norm": 0.6421489715576172,
      "learning_rate": 9.387533875338754e-05,
      "loss": 0.082,
      "step": 980
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 0.6110385060310364,
      "learning_rate": 9.279132791327913e-05,
      "loss": 0.0784,
      "step": 990
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 0.4665641784667969,
      "learning_rate": 9.170731707317075e-05,
      "loss": 0.0838,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1845,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.8452488126464e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
